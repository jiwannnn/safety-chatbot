# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Mnsdmkw5XqZmEHJTvGn7gp2pm4-qhiQ
"""
# Streamlit í˜ì´ì§€ ì„¤ì •
import streamlit as st
st.set_page_config(page_title="ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ", page_icon="ğŸ¤–")

# ì‚¬ìš©ì ì§€ì • CSS ì¶”ê°€
st.markdown(
    """
    <style>
    body {
        background-color: #f4f4f4;
    }
    .top-right {
        position: fixed;
        top: 10px;
        right: 10px;
        font-size: 14px;
        color: #555;
        font-family: Arial, sans-serif;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.markdown('<div class="top-right">by ê¹€ì§€ì™„</div>', unsafe_allow_html=True)

import os
import time
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader, CSVLoader

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ì—…ì¢…ë³„ íŒŒì¼ ì„¤ì •
industry_files = {
    "ê°•ì„ _ê±´ì¡°ì—…_ì•ˆì „ë³´ê±´": ["./data/ê°•ì„  ê±´ì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ ë°•ê±´ì¡° ë° ìˆ˜ë¦¬ì—….csv"],
    "ë²Œëª©ì—…_ì•ˆì „ë³´ê±´": ["./data/ë²Œëª©ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ì—….csv"],
    "ì„¬ìœ _ë°_ì„¬ìœ ì œí’ˆ_ì œì¡°ì—…(í‘œë°±_ë°_ì—¼ìƒ‰ê°€ê³µì—…)": ["./data/ì„¬ìœ ì œí’ˆ ì—¼ìƒ‰, ì •ë¦¬ ë° ë§ˆë¬´ë¦¬ ê°€ê³µì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„¬ìœ  ë° ì„¬ìœ ì œí’ˆì œì¡°ì—….csv"],
    "ì¸ì‡„ì—…_ì•ˆì „ë³´ê±´": ["./data/ì¸ì‡„ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì¶œíŒ ì¸ì‡„ì—….csv"],
    "í”Œë¼ìŠ¤í‹±ì œí’ˆ_ì•ˆì „ë³´ê±´": ["./data/í”Œë¼ìŠ¤í‹± ì œí’ˆ ì œì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/í”Œë¼ìŠ¤í‹± ê°€ê³µ ì œí’ˆì œì¡°ì—….csv"],
    "ìë™ì°¨ë¶€í’ˆ_ì•ˆì „ë³´ê±´": ["./data/ìë™ì°¨ ë¶€í’ˆ ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì°¨ëŸ‰ ë¶€í’ˆ ì œì¡°ì—….csv"],
}

common_file_path = "./data/ê³µí†µ.csv"

# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
def create_text_splitter(context_length=None):
    if context_length and context_length > 20000:
        return CharacterTextSplitter(chunk_size=150, chunk_overlap=20)
    return CharacterTextSplitter(chunk_size=200, chunk_overlap=30)

# ì¼ë°˜ íŒŒì¼ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
def create_vector_store(files, embeddings, source_type):
    all_documents = []
    for name, file_paths in files.items():
        if not isinstance(file_paths, list):
            file_paths = [file_paths]

        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    st.warning(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                    continue

                loader = TextLoader(file_path, encoding="utf-8")
                documents = loader.load()

                for doc in documents:
                    doc.metadata["source"] = name
                    doc.metadata["type"] = source_type

                all_documents.extend(documents)
            except Exception as e:
                st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}\n{str(e)}")

    text_splitter = create_text_splitter(len(" ".join([doc.page_content for doc in all_documents]).split()))
    split_texts = text_splitter.split_documents(all_documents)
    return FAISS.from_documents(split_texts, embeddings)

# CSV íŒŒì¼ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
def create_vector_store_from_csv(file_path, embeddings, source_type):
    try:
        loader = CSVLoader(file_path=file_path, encoding="utf-8")
        documents = loader.load()

        for doc in documents:
            doc.metadata["source"] = source_type

        return FAISS.from_documents(documents, embeddings)
    except Exception as e:
        st.error(f"CSV ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

embeddings = OpenAIEmbeddings()

try:
    industry_vector_store = create_vector_store(industry_files, embeddings, "industry")
    common_vector_store = create_vector_store_from_csv(common_file_path, embeddings, "common")
except Exception as e:
    st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

st.markdown("<h1 style='text-align: center;'>ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ</h1>", unsafe_allow_html=True)

selected_industry = st.selectbox("ì—…ì¢…ì„ ì„ íƒí•˜ì„¸ìš”", list(industry_files.keys()))
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ê²€ìƒ‰"):
    if not query:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        try:
            industry_retriever = industry_vector_store.as_retriever(search_kwargs={"k": 1})
            industry_results = industry_retriever.get_relevant_documents(query)

            common_retriever = common_vector_store.as_retriever(search_kwargs={"k": 1})
            common_results = common_retriever.get_relevant_documents(query)

            all_results = industry_results + common_results
            combined_context = "\n".join([doc.page_content for doc in all_results])

            text_splitter = create_text_splitter(len(combined_context.split()))
            split_contexts = text_splitter.split_text(combined_context)

            # ìµœëŒ€ ìš”ì²­í•  ì²­í¬ ì œí•œ (ì˜ˆ: ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ ì‚¬ìš©)
            split_contexts = split_contexts[:2]

            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
            {context}
            ì§ˆë¬¸: {question}
            ë‹µë³€:"""

            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)
            llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, max_tokens=150)

            final_response = ""
            for chunk in split_contexts:
                chain = LLMChain(llm=llm, prompt=prompt)
                response = chain.run({"context": chunk, "question": query})
                final_response += response + "\n"
                time.sleep(2)

            st.subheader("ë‹µë³€")
            st.write(final_response)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
