# -*- coding: utf-8 -*-
import streamlit as st
import os
import time
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ", page_icon="ğŸ¤–")

st.markdown(
    """
    <style>
    body {
        background-color: #f4f4f4;
    }
    .bottom-right {
        position: fixed;
        bottom: 10px;
        right: 10px;
        font-size: 14px;
        color: #555;
        font-family: Arial, sans-serif;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.markdown('<div class="bottom-right">by ê¹€ì§€ì™„</div>', unsafe_allow_html=True)

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ì—…ì¢…ë³„ íŒŒì¼ ì„¤ì •
industry_files = {
    "ê°•ì„ _ê±´ì¡°ì—…_ì•ˆì „ë³´ê±´": ["./data/ê°•ì„  ê±´ì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ ë°•ê±´ì¡° ë° ìˆ˜ë¦¬ì—….csv"],
    "ë²Œëª©ì—…_ì•ˆì „ë³´ê±´": ["./data/ë²Œëª©ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ì—….csv"],
    "ì„¬ìœ _ë°_ì„¬ìœ ì œí’ˆ_ì œì¡°ì—…(í‘œë°±_ë°_ì—¼ìƒ‰ê°€ê³µì—…)": ["./data/ì„¬ìœ ì œí’ˆ ì—¼ìƒ‰, ì •ë¦¬ ë° ë§ˆë¬´ë¦¬ ê°€ê³µì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„¬ìœ  ë° ì„¬ìœ ì œí’ˆì œì¡°ì—….csv"],
    "ì¸ì‡„ì—…_ì•ˆì „ë³´ê±´": ["./data/ì¸ì‡„ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì¶œíŒ ì¸ì‡„ì—….csv"],
    "í”Œë¼ìŠ¤í‹±ì œí’ˆ_ì•ˆì „ë³´ê±´": ["./data/í”Œë¼ìŠ¤í‹± ì œí’ˆ ì œì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/í”Œë¼ìŠ¤í‹± ê°€ê³µ ì œí’ˆì œì¡°ì—….csv"],
    "ìë™ì°¨ë¶€í’ˆ_ì•ˆì „ë³´ê±´": ["./data/ìë™ì°¨ ë¶€í’ˆ ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì°¨ëŸ‰ ë¶€í’ˆ ì œì¡°ì—….csv"],
}

common_file_path = "./data/ê³µí†µ.csv"

# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
def create_text_splitter(context_length=None):
    # ê¸°ë³¸ ë¶„í•  ì„¤ì •
    chunk_size = 200
    chunk_overlap = 50

    if context_length and context_length > 32000:
        chunk_size = 150
        chunk_overlap = 50

    # RecursiveCharacterTextSplitter ì‚¬ìš©
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],  # êµ¬ë¶„ì ìˆœì„œëŒ€ë¡œ ì‹œë„
    )

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
def create_vector_store(files, embeddings, source_type):
    all_documents = []
    for name, file_paths in files.items():
        if not isinstance(file_paths, list):
            file_paths = [file_paths]

        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    st.warning(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                    continue

                loader = TextLoader(file_path, encoding="utf-8")
                documents = loader.load()

                for doc in documents:
                    doc.metadata["source"] = name
                    doc.metadata["type"] = source_type

                all_documents.extend(documents)
            except Exception as e:
                st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}\n{str(e)}")

    text_splitter = create_text_splitter(len(" ".join([doc.page_content for doc in all_documents]).split()))
    split_texts = text_splitter.split_documents(all_documents)
    return FAISS.from_documents(split_texts, embeddings)

# ìš”ì•½ í•¨ìˆ˜ (GPT-4-32k ì‚¬ìš©)
def summarize_context(llm, context):
    prompt = PromptTemplate(
        input_variables=["context"],
        template=(
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê°„ëµíˆ ìš”ì•½í•˜ì„¸ìš”:\n\n"
            "{context}\n\n"
            "ìš”ì•½:"
        )
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    max_context_length = 20000  # ì•ˆì „í•œ ì…ë ¥ ê¸¸ì´ ì œí•œ
    truncated_context = context[:max_context_length]
    summary = chain.run({"context": truncated_context})
    return summary

# OpenAI ì„ë² ë”© ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings()

try:
    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    industry_vector_store = create_vector_store(industry_files, embeddings, "industry")
    common_vector_store = create_vector_store({"ê³µí†µ ì‚¬ë¡€": common_file_path}, embeddings, "common")
except Exception as e:
    st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# Streamlit UI êµ¬ì„±
st.markdown("<h1 style='text-align: center;'>ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ</h1>", unsafe_allow_html=True)

selected_industry = st.selectbox("ì—…ì¢…ì„ ì„ íƒí•˜ì„¸ìš”", list(industry_files.keys()))
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ê²€ìƒ‰"):
    if not query:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            industry_retriever = industry_vector_store.as_retriever(search_kwargs={"k": 1})
            industry_results = industry_retriever.get_relevant_documents(query)

            common_retriever = common_vector_store.as_retriever(search_kwargs={"k": 1})
            common_results = common_retriever.get_relevant_documents(query)

            all_results = industry_results + common_results
            combined_context = "\n".join([doc.page_content for doc in all_results])

            # ìš”ì•½ ë‹¨ê³„
            llm_summary = ChatOpenAI(model_name="gpt-4-turbo-32k", temperature=0, max_tokens=1000)
            summarized_context = summarize_context(llm_summary, combined_context)

            # ìµœì¢… ë‹µë³€ ìƒì„±
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
            {context}
            ì§ˆë¬¸: {question}
            ë‹µë³€:"""

            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)
            llm = ChatOpenAI(model_name="gpt-4-32k", temperature=0, max_tokens=500)

            chain = LLMChain(llm=llm, prompt=prompt)

            # **ì²­í¬ í¬ê¸°ë¥¼ ì¤„ì´ê³  ìš”ì²­ ê°„ ë”œë ˆì´ ì¶”ê°€**
            final_response = ""
            for chunk in summarized_context.split("\n"):
                if chunk.strip():  # ë¹ˆ ì²­í¬ ì œì™¸
                    response = chain.run({"context": chunk, "question": query})
                    final_response += response + "\n"
                    time.sleep(2)  # ìš”ì²­ ê°„ 2ì´ˆ ëŒ€ê¸°

            # ê²°ê³¼ ì¶œë ¥
            st.subheader("ë‹µë³€")
            st.write(final_response)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

