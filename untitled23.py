# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Mnsdmkw5XqZmEHJTvGn7gp2pm4-qhiQ
"""
# Streamlit í˜ì´ì§€ ì„¤ì • (í•­ìƒ ìµœìƒë‹¨ì— ìœ„ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤)
import streamlit as st

st.set_page_config(page_title="ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ", page_icon="ğŸ¤–")

# ì‚¬ìš©ì ì§€ì • CSS ì¶”ê°€
st.markdown(
    """
    <style>
    /* ì „ì²´ ë°°ê²½ ìƒ‰ */
    body {
        background-color: #f4f4f4;
    }

    /* ì±„íŒ… UI ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 700px;
        margin: 0 auto;
        padding: 10px;
    }
    .user-message {
        text-align: right;
        background-color: #e6f7ff;
        padding: 10px;
        border-radius: 10px;
        margin-bottom: 10px;
        font-family: Arial, sans-serif;
    }
    .ai-message {
        text-align: left;
        background-color: #f9f9f9;
        padding: 10px;
        border-radius: 10px;
        margin-bottom: 10px;
        font-family: Arial, sans-serif;
    }

    /* ìƒë‹¨ "by ê¹€ì§€ì™„" ìŠ¤íƒ€ì¼ */
    .top-right {
        position: fixed;
        top: 10px;
        right: 10px;
        font-size: 14px;
        color: #555;
        font-family: Arial, sans-serif;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ìƒë‹¨ "by ê¹€ì§€ì™„" í‘œì‹œ
st.markdown('<div class="top-right">by ê¹€ì§€ì™„</div>', unsafe_allow_html=True)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import time
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ì—…ì¢…ë³„ íŒŒì¼ ì„¤ì •
industry_files = {
    "ê°•ì„ _ê±´ì¡°ì—…_ì•ˆì „ë³´ê±´": ["./data/ê°•ì„  ê±´ì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ ë°•ê±´ì¡° ë° ìˆ˜ë¦¬ì—….csv"],
    "ë²Œëª©ì—…_ì•ˆì „ë³´ê±´": ["./data/ë²Œëª©ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ì—….csv"],
    "ì„¬ìœ _ë°_ì„¬ìœ ì œí’ˆ_ì œì¡°ì—…(í‘œë°±_ë°_ì—¼ìƒ‰ê°€ê³µì—…)": ["./data/ì„¬ìœ ì œí’ˆ ì—¼ìƒ‰, ì •ë¦¬ ë° ë§ˆë¬´ë¦¬ ê°€ê³µì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„¬ìœ  ë° ì„¬ìœ ì œí’ˆì œì¡°ì—….csv"],
    "ì¸ì‡„ì—…_ì•ˆì „ë³´ê±´": ["./data/ì¸ì‡„ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì¶œíŒ ì¸ì‡„ì—….csv"],
    "í”Œë¼ìŠ¤í‹±ì œí’ˆ_ì•ˆì „ë³´ê±´": ["./data/í”Œë¼ìŠ¤í‹± ì œí’ˆ ì œì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/í”Œë¼ìŠ¤í‹± ê°€ê³µ ì œí’ˆì œì¡°ì—….csv"],
    "ìë™ì°¨ë¶€í’ˆ_ì•ˆì „ë³´ê±´": ["./data/ìë™ì°¨ ë¶€í’ˆ ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì°¨ëŸ‰ ë¶€í’ˆ ì œì¡°ì—….csv"],
}

# ê³µí†µ ì‚¬ë¡€ íŒŒì¼ ê²½ë¡œ
common_file_path = "./data/ê³µí†µ.csv"

# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
def create_text_splitter():
    return CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

text_splitter = create_text_splitter()

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜
def create_vector_store(files, embeddings, source_type):
    all_documents = []
    for name, file_paths in files.items():
        if not isinstance(file_paths, list):
            file_paths = [file_paths]

        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    st.warning(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                    continue

                # í…ìŠ¤íŠ¸ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¡œë“œ
                loader = TextLoader(file_path, encoding="utf-8")
                documents = loader.load()

                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for doc in documents:
                    doc.metadata["source"] = name
                    doc.metadata["type"] = source_type

                # í…ìŠ¤íŠ¸ ë¶„í• 
                split_texts = text_splitter.split_documents(documents)
                all_documents.extend(split_texts)
            except Exception as e:
                st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}\n{str(e)}")

    st.info(f"ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents)}")
    return FAISS.from_documents(all_documents, embeddings)

# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings()
try:
    industry_vector_store = create_vector_store(industry_files, embeddings, "industry")
    common_vector_store = create_vector_store({"ê³µí†µ ì‚¬ë¡€": common_file_path}, embeddings, "common")
except Exception as e:
    st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# Streamlit ì•± ì œëª©
st.markdown(
    """
    <h1 style="text-align: center; color: black;">
        ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ
    </h1>
    """,
    unsafe_allow_html=True
)

# ì—…ì¢… ì„ íƒ
selected_industry = st.selectbox("ì—…ì¢…ì„ ì„ íƒí•˜ì„¸ìš”", list(industry_files.keys()))

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

# ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰
if st.button("ê²€ìƒ‰"):
    if not query:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        try:
            # ì—…ì¢… ë°ì´í„° ê²€ìƒ‰
            industry_retriever = industry_vector_store.as_retriever(search_kwargs={"k": 3})
            industry_results = industry_retriever.get_relevant_documents(query)

            # ê³µí†µ ë°ì´í„° ê²€ìƒ‰
            common_retriever = common_vector_store.as_retriever(search_kwargs={"k": 3})
            common_results = common_retriever.get_relevant_documents(query)

            # ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
            all_results = industry_results + common_results
            combined_context = "\n".join([doc.page_content for doc in all_results])

            # í…ìŠ¤íŠ¸ ë¶„í• 
            split_contexts = text_splitter.split_text(combined_context)

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

            {context}

            ì§ˆë¬¸: {question}
            ë‹µë³€:"""

            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

            # GPT-4 ëª¨ë¸ ì„¤ì •
            llm = ChatOpenAI(model_name="gpt-4", temperature=0, max_tokens=500)

            # ë¶„í• ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ìƒì„±
            final_response = ""
            for chunk in split_contexts:
                chain = LLMChain(llm=llm, prompt=prompt)
                response = chain.run({"context": chunk, "question": query})
                final_response += response + "\n"
                time.sleep(2)

            # ê²°ê³¼ ì¶œë ¥
            st.subheader("ë‹µë³€")
            st.write(final_response)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

