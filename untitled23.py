# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Mnsdmkw5XqZmEHJTvGn7gp2pm4-qhiQ
"""

import os
import time
import streamlit as st
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "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"  # ì—¬ê¸°ì— ë³¸ì¸ì˜ OpenAI API í‚¤ë¥¼ ì…ë ¥

# ì—…ì¢…ë³„ íŒŒì¼ ì„¤ì •
industry_files = {
    "ê°•ì„ _ê±´ì¡°ì—…_ì•ˆì „ë³´ê±´": ["./data/ê°•ì„  ê±´ì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ ë°•ê±´ì¡° ë° ìˆ˜ë¦¬ì—….csv"],
    "ë²Œëª©ì—…_ì•ˆì „ë³´ê±´": ["./data/ë²Œëª©ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ì—….csv"],
    "ì„¬ìœ _ë°_ì„¬ìœ ì œí’ˆ_ì œì¡°ì—…(í‘œë°±_ë°_ì—¼ìƒ‰ê°€ê³µì—…)": ["./data/ì„¬ìœ ì œí’ˆ ì—¼ìƒ‰, ì •ë¦¬ ë° ë§ˆë¬´ë¦¬ ê°€ê³µì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„¬ìœ  ë° ì„¬ìœ ì œí’ˆì œì¡°ì—….csv"],
    "ì¸ì‡„ì—…_ì•ˆì „ë³´ê±´": ["./data/ì¸ì‡„ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì¶œíŒ ì¸ì‡„ì—….csv"],
    "í”Œë¼ìŠ¤í‹±ì œí’ˆ_ì•ˆì „ë³´ê±´": ["./data/í”Œë¼ìŠ¤í‹± ì œí’ˆ ì œì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/í”Œë¼ìŠ¤í‹± ê°€ê³µ ì œí’ˆì œì¡°ì—….csv"],
    "ìë™ì°¨ë¶€í’ˆ_ì•ˆì „ë³´ê±´": ["./data/ìë™ì°¨ ë¶€í’ˆ ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì°¨ëŸ‰ ë¶€í’ˆ ì œì¡°ì—….csv"],
}

# ê³µí†µ ì‚¬ë¡€ íŒŒì¼ ê²½ë¡œ
common_file_path = "./data/ê³µí†µ.csv"

# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
def create_text_splitter():
    return CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

text_splitter = create_text_splitter()

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜
def create_vector_store(files, embeddings, source_type):
    all_documents = []
    for name, file_paths in files.items():
        if not isinstance(file_paths, list):  # íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            file_paths = [file_paths]

        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    st.warning(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                    continue
                
                # í…ìŠ¤íŠ¸ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¡œë“œ
                loader = TextLoader(file_path, encoding="utf-8")
                documents = loader.load()

                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for doc in documents:
                    doc.metadata["source"] = name
                    doc.metadata["type"] = source_type

                # í…ìŠ¤íŠ¸ ë¶„í• 
                split_texts = text_splitter.split_documents(documents)
                all_documents.extend(split_texts)
            except Exception as e:
                st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}\n{str(e)}")

    st.info(f"ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents)}")
    return Chroma.from_documents(all_documents, embeddings)

# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” (ì—…ì¢…ë³„ ë° ê³µí†µ ì‚¬ë¡€)
embeddings = OpenAIEmbeddings()
try:
    industry_vector_store = create_vector_store(industry_files, embeddings, "industry")
    common_vector_store = create_vector_store({"ê³µí†µ ì‚¬ë¡€": common_file_path}, embeddings, "common")
except Exception as e:
    st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ì§ˆì˜ì‘ë‹µ", page_icon="ğŸ¤–")
st.title("ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

# ì—…ì¢… ì„ íƒ
selected_industry = st.selectbox("ì—…ì¢…ì„ ì„ íƒí•˜ì„¸ìš”", list(industry_files.keys()))

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

# ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰
if st.button("ê²€ìƒ‰"):
    if not query:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        try:
            # ì—…ì¢… ë°ì´í„° ê²€ìƒ‰
            industry_retriever = industry_vector_store.as_retriever(search_kwargs={"k": 3})
            industry_results = industry_retriever.get_relevant_documents(query)

            # ê³µí†µ ë°ì´í„° ê²€ìƒ‰
            common_retriever = common_vector_store.as_retriever(search_kwargs={"k": 3})
            common_results = common_retriever.get_relevant_documents(query)

            # ì„ íƒëœ ì—…ì¢…ì˜ ë°ì´í„°ì™€ ê³µí†µ ë°ì´í„°ë¥¼ í•¨ê»˜ ê²°í•©
            all_results = industry_results + common_results

            # ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
            combined_context = "\n".join([doc.page_content for doc in all_results])

            # í…ìŠ¤íŠ¸ ë¶„í• 
            split_contexts = text_splitter.split_text(combined_context)

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

            {context}

            ì§ˆë¬¸: {question}
            ë‹µë³€:"""

            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

            # GPT-4 ëª¨ë¸ ì„¤ì •
            llm = ChatOpenAI(model_name="gpt-4", temperature=0, max_tokens=500)

            # ë¶„í• ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ìƒì„±
            final_response = ""
            for chunk in split_contexts:
                chain = LLMChain(llm=llm, prompt=prompt)
                response = chain.run({"context": chunk, "question": query})
                final_response += response + "\n"
                time.sleep(2)  # ì†ë„ ì œí•œ ë°©ì§€

            # ê²°ê³¼ ì¶œë ¥
            st.subheader("ë‹µë³€")
            st.write(final_response)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
