# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Mnsdmkw5XqZmEHJTvGn7gp2pm4-qhiQ
"""
# Streamlit í˜ì´ì§€ ì„¤ì •
import streamlit as st
st.set_page_config(page_title="ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ", page_icon="ğŸ¤–")

# ì‚¬ìš©ì ì§€ì • CSS ì¶”ê°€
st.markdown(
    """
    <style>
    body {
        background-color: #f4f4f4;
    }
    .top-right {
        position: fixed;
        top: 10px;
        right: 10px;
        font-size: 14px;
        color: #555;
        font-family: Arial, sans-serif;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.markdown('<div class="top-right">by ê¹€ì§€ì™„</div>', unsafe_allow_html=True)

import os
import time
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ì—…ì¢…ë³„ íŒŒì¼ ì„¤ì •
industry_files = {
    "ê°•ì„ _ê±´ì¡°ì—…_ì•ˆì „ë³´ê±´": ["./data/ê°•ì„  ê±´ì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ ë°•ê±´ì¡° ë° ìˆ˜ë¦¬ì—….csv"],
    "ë²Œëª©ì—…_ì•ˆì „ë³´ê±´": ["./data/ë²Œëª©ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„ì—….csv"],
    "ì„¬ìœ _ë°_ì„¬ìœ ì œí’ˆ_ì œì¡°ì—…(í‘œë°±_ë°_ì—¼ìƒ‰ê°€ê³µì—…)": ["./data/ì„¬ìœ ì œí’ˆ ì—¼ìƒ‰, ì •ë¦¬ ë° ë§ˆë¬´ë¦¬ ê°€ê³µì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì„¬ìœ  ë° ì„¬ìœ ì œí’ˆì œì¡°ì—….csv"],
    "ì¸ì‡„ì—…_ì•ˆì „ë³´ê±´": ["./data/ì¸ì‡„ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì¶œíŒ ì¸ì‡„ì—….csv"],
    "í”Œë¼ìŠ¤í‹±ì œí’ˆ_ì•ˆì „ë³´ê±´": ["./data/í”Œë¼ìŠ¤í‹± ì œí’ˆ ì œì¡°ì—… ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/í”Œë¼ìŠ¤í‹± ê°€ê³µ ì œí’ˆì œì¡°ì—….csv"],
    "ìë™ì°¨ë¶€í’ˆ_ì•ˆì „ë³´ê±´": ["./data/ìë™ì°¨ ë¶€í’ˆ ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œ.md", "./data/ì°¨ëŸ‰ ë¶€í’ˆ ì œì¡°ì—….csv"],
}

common_file_path = "./data/ê³µí†µ.csv"

# í…ìŠ¤íŠ¸ ìš”ì•½ í•¨ìˆ˜
def summarize_context(llm, context):
    prompt = PromptTemplate(
        input_variables=["context"],
        template="ë‹¤ìŒ ë¬¸ì„œë¥¼ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n\n{context}\n\nìš”ì•½:"
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    summary = chain.run({"context": context[:2000]})  # ìš”ì•½ ì‹œ ì…ë ¥ í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ì œí•œ
    return summary

# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
def create_text_splitter(context_length=None):
    if context_length and context_length > 10000:
        return CharacterTextSplitter(chunk_size=100, chunk_overlap=10)  # ë” ì‘ì€ ì²­í¬
    return CharacterTextSplitter(chunk_size=150, chunk_overlap=20)

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
def create_vector_store(files, embeddings, source_type):
    all_documents = []
    for name, file_paths in files.items():
        if not isinstance(file_paths, list):
            file_paths = [file_paths]

        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    st.warning(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                    continue

                loader = TextLoader(file_path, encoding="utf-8")
                documents = loader.load()

                for doc in documents:
                    doc.metadata["source"] = name
                    doc.metadata["type"] = source_type

                all_documents.extend(documents)
            except Exception as e:
                st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}\n{str(e)}")

    text_splitter = create_text_splitter(len(" ".join([doc.page_content for doc in all_documents]).split()))
    split_texts = text_splitter.split_documents(all_documents)
    return FAISS.from_documents(split_texts, embeddings)

embeddings = OpenAIEmbeddings()

try:
    industry_vector_store = create_vector_store(industry_files, embeddings, "industry")
    common_vector_store = create_vector_store({"ê³µí†µ ì‚¬ë¡€": common_file_path}, embeddings, "common")
except Exception as e:
    st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

st.markdown("<h1 style='text-align: center;'>ì—…ì¢…ë³„ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€ ë° ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„ ì§ˆì˜ì‘ë‹µ</h1>", unsafe_allow_html=True)

selected_industry = st.selectbox("ì—…ì¢…ì„ ì„ íƒí•˜ì„¸ìš”", list(industry_files.keys()))
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ê²€ìƒ‰"):
    if not query:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        try:
            industry_retriever = industry_vector_store.as_retriever(search_kwargs={"k": 1})  # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ë¥¼ ì œí•œ
            industry_results = industry_retriever.get_relevant_documents(query)

            common_retriever = common_vector_store.as_retriever(search_kwargs={"k": 1})  # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ë¥¼ ì œí•œ
            common_results = common_retriever.get_relevant_documents(query)

            # ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
            all_results = industry_results + common_results
            combined_context = "\n".join([doc.page_content for doc in all_results])

            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            llm_summary = ChatOpenAI(model_name="gpt-4", temperature=0, max_tokens=300)  # ìš”ì•½ìš© ëª¨ë¸
            summarized_context = summarize_context(llm_summary, combined_context[:5000])  # ìš”ì•½ ì‹œ ì…ë ¥ í¬ê¸° ì œí•œ

            # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì„¤ì • ë° ë‹µë³€ ìƒì„±
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
            {context}
            ì§ˆë¬¸: {question}
            ë‹µë³€:"""

            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)
            llm = ChatOpenAI(model_name="gpt-4", temperature=0, max_tokens=150)

            chain = LLMChain(llm=llm, prompt=prompt)
            final_response = chain.run({"context": summarized_context, "question": query})

            st.subheader("ë‹µë³€")
            st.write(final_response)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
